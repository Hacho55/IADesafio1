{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CQRE1wzr2z0T"
   },
   "outputs": [],
   "source": [
    "#Descomentar las siguientes líneas para instalar gensim y smart_open\n",
    "#conda install -c anaconda gensim\n",
    "#conda install smart_open==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANTE: antes de ejecutar la notebooks, descomprimir Data/Fake.rar y Data/True.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Desktop\\Courses\\inteligencia-artificial\\fake-news-classification\\Repositorio\\Funciones.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "%run \"Funciones.py\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVqUxnL544YF"
   },
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Data/Fake.csv')\n",
    "fake['label'] = 1\n",
    "real = pd.read_csv('Data/True.csv')\n",
    "real['label'] = 0\n",
    "\n",
    "df = pd.concat([fake, real], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhGLiFiLm-Vl"
   },
   "source": [
    "# Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyFHmog1mnBQ"
   },
   "outputs": [],
   "source": [
    "total_entradas = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auUc8Xd79f7i"
   },
   "source": [
    "## Vemos si el dataset está balanceado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "ohFZ4-9o8iMx",
    "outputId": "5d1a07c8-b221-454c-b0b9-ac34c2f2ba86"
   },
   "outputs": [],
   "source": [
    "graficoBalanceado()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IWnrEb08oVL"
   },
   "source": [
    "## Veamos si existen entradas duplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4V1XwCW27Xl"
   },
   "source": [
    "## Entradas duplicadas por título"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "e01lNS4zhITT",
    "outputId": "0c7c08f6-fa28-4cbc-b37c-7b8d3e51695c"
   },
   "outputs": [],
   "source": [
    "unique_q = df.groupby([\"title\", 'label']).filter(lambda x: len(x) == 1)\n",
    "duplicate_q = df.groupby([\"title\", 'label']).filter(lambda x: len(x) > 1)\n",
    "\n",
    "graficarPie(\n",
    "    (len(unique_q)*100/total_entradas, len(duplicate_q)*100/total_entradas), \n",
    "    ['Únicas por title', 'Duplicadas por title'], \n",
    "    \"%1.1f%%\", \n",
    "    'Graficos/02_TitulosDuplicados');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMu8ZC9k2-wD"
   },
   "source": [
    "## Entradas duplicadas por texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "VZsf0Xb2l6pf",
    "outputId": "ca18eb7f-2e8e-461b-954c-3eb23fe6dc91"
   },
   "outputs": [],
   "source": [
    "unique_q = df.groupby([\"text\", 'label']).filter(lambda x: len(x) == 1)\n",
    "duplicate_q = df.groupby([\"text\", 'label']).filter(lambda x: len(x) > 1)\n",
    "graficarPie(\n",
    "    (len(unique_q)*100/total_entradas, len(duplicate_q)*100/total_entradas),\n",
    "    ['Únicas por text', 'Duplicadas por text'],\n",
    "    '%1.1f%%',\n",
    "    'Graficos/03_TextosDuplicados');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOBfquTh3Bg5"
   },
   "source": [
    "## Entradas duplicadas por título y texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "arT3eX1xlvKL",
    "outputId": "cce1f007-d749-4eeb-8cda-87e23f912e90"
   },
   "outputs": [],
   "source": [
    "unique_q = df.groupby(['text', 'title', 'label']).filter(lambda x: len(x) == 1)\n",
    "duplicate_q = df.groupby(['text', 'title', 'label']).filter(lambda x: len(x) > 1)\n",
    "graficarPie(\n",
    "    (len(unique_q)*100/total_entradas, len(duplicate_q)*100/total_entradas),\n",
    "    ['Únicas por title+text', 'Duplicadas por title+text'],\n",
    "    '%1.1f%%',\n",
    "    'Graficos/04_TitulosTextosDuplicados');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6otDBXdnEoj"
   },
   "source": [
    "## Veamos la distribución de caracteres y palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3wAVSYmnOlc"
   },
   "source": [
    "## Por titulo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "ie0Es3c3mz2n",
    "outputId": "ef9769b9-a4a3-4fdf-ba47-89ac3f75cef9"
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "df['length'] = df['title'].apply(len)\n",
    "df['length_tokens'] = df['title'].apply(lambda x : len(tokenizer.tokenize(x)))\n",
    "\n",
    "fig, axes = matplotlib.pyplot.subplots(figsize=(14, 6), nrows=1, ncols=2)\n",
    "\n",
    "df['length'].hist(bins=80, ax=axes[0]);\n",
    "df['length_tokens'].hist(bins=80, ax=axes[1]);\n",
    "\n",
    "axes[0].title.set_text('Distribución de caracteres del título');\n",
    "axes[1].title.set_text('Distribución de palabras del título');\n",
    "\n",
    "fig.savefig('Graficos/05_DistribucionCaracteresTitulo.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "IJcAky5LnUlq",
    "outputId": "7f806cf0-2bea-4948-ca07-29b8751ac995"
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "df['length'] = df['text'].apply(len)\n",
    "df['length_tokens'] = df['text'].apply(lambda x : len(tokenizer.tokenize(x)))\n",
    "\n",
    "fig, axes = matplotlib.pyplot.subplots(figsize=(14, 6), nrows=1, ncols=2)\n",
    "\n",
    "df['length'].hist(bins=121, ax=axes[0]);\n",
    "df['length_tokens'].hist(bins=80, ax=axes[1]);\n",
    "\n",
    "axes[0].title.set_text('Distribución de caracteres del texto');\n",
    "axes[1].title.set_text('Distribución de palabras del texto');\n",
    "\n",
    "fig.savefig('Graficos/05_DistribucionCaracteresTexto.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO4kTqq1nwWI"
   },
   "source": [
    "## Palabras más frecuentes y menos frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "CKS8tum2nfM2",
    "outputId": "5d4a3d82-a39b-465b-8225-59163117595a"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "from collections import Counter\n",
    "df['texto'] = df['title'] + \" \" + df['text']\n",
    "df['tokens'] = df['texto'].apply(lambda x : tokenizer.tokenize(x))\n",
    "dic = Counter()\n",
    "\n",
    "for tokens in df.tokens.values:\n",
    "  dic.update(tokens)\n",
    "\n",
    "def getWordcloud(data):\n",
    "  return WordCloud(background_color ='white', min_font_size = 10).generate_from_frequencies(data)\n",
    "\n",
    "wordcloud1 = getWordcloud(dict(dic.most_common(30)))\n",
    "wordcloud2 = getWordcloud(dict(dic.most_common()[-30:]))\n",
    "\n",
    "fig, axes = matplotlib.pyplot.subplots(figsize=(15, 15), nrows=1, ncols=2)\n",
    "axes[0].imshow(wordcloud1, interpolation='bilinear')\n",
    "axes[1].imshow(wordcloud2, interpolation='bilinear')\n",
    "\n",
    "axes[0].title.set_text('Palabras más frecuentes')\n",
    "axes[1].title.set_text('Palabras menos frecuentes')\n",
    "\n",
    "fig.savefig('Graficos/06_WorldCount.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYToK0eN3JJU"
   },
   "source": [
    "# Utilizar herramientas avanzadas para conocer la relación entre las palabras (palabras similares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbrzpOtVn98D"
   },
   "outputs": [],
   "source": [
    "model_words = gensim.models.FastText(sentences=df.tokens.values, min_count=5, max_vocab_size=7000,size=32, workers=3, window=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "-LpY4kvgoRNZ",
    "outputId": "cc45408c-985d-46d3-afd5-95bbcf281ebe"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model_words.wv.most_similar('trump', topn=5), columns=['Palabra', 'Relación'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "T9Cyt-Y4oS72",
    "outputId": "08e21f19-c1c2-4e84-8425-a40f78430128"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model_words.wv.most_similar('FBI', topn=5), columns=['Palabra', 'Relación'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "w-Hg-j-RuX0X",
    "outputId": "3e88d870-c79a-4fd3-a9ec-d16f864ee8cf"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model_words.wv.most_similar('Reuters', topn=10), columns=['Palabra', 'Relación'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "8ns2MlKuxY_D",
    "outputId": "0197b13d-e589-4b5b-c389-db6ccdb9a90a"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model_words.wv.most_similar('news', topn=10), columns=['Palabra', 'Relación'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "HEMn-GKMygib",
    "outputId": "4346bfaa-fb79-445b-84d8-7225ce4c91d9"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(model_words.wv.most_similar('breaking', topn=10), columns=['Palabra', 'Relación'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKKRI4nzA5JU"
   },
   "source": [
    "## Palabras comunes usando un Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "mEUMRzOtA4OH",
    "outputId": "0f63874d-06fd-4f7d-ed28-ae9d8f6b3369"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "splited = df['text'].str.split()\n",
    "counter_words = Counter(list(chain(*splited)))\n",
    "counter_frame = pd.DataFrame(data=counter_words, index=['count'])\n",
    "top_common_words = counter_frame.T.sort_values(by=['count'], ascending=False).reset_index()\n",
    "top_common_words.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBkszpv6CvZp"
   },
   "source": [
    "## Sobre otras columnas (subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "GoIeBEmtCxK6",
    "outputId": "6ab48c5b-e66b-42f6-d642-8421bc91b82c"
   },
   "outputs": [],
   "source": [
    "df_subjects = df.groupby(['subject', 'label'])['text'].count()\n",
    "df_subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFN632d-_MFt"
   },
   "source": [
    "# Posibles conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InIQBNcT_POC"
   },
   "source": [
    "\n",
    "\n",
    "*   El dataset está balanceado\n",
    "*   Hay que eliminar stop words frecuentes\n",
    "*   Hay que eliminar las palabras menos frecuentes\n",
    "*   Los textos tienen como máximo 2000 palabras\n",
    "*   Los títulos tienen como máximo 40 palabras\n",
    "*   Subject divide exactamente en fake o real el dataset\n",
    "*   Las palabras más frecuentes son stopwords"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exploración DH Reto1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
